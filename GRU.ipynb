{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
      "\u001b[K     |████████████████████████████████| 734.6MB 48kB/s s eta 0:00:01    |█▉                              | 42.3MB 22.9MB/s eta 0:00:31     |███████████▏                    | 255.3MB 29.0MB/s eta 0:00:17     |██████████████▍                 | 331.3MB 43.9MB/s eta 0:00:10     |███████████████▌                | 355.3MB 43.9MB/s eta 0:00:09     |████████████████▍               | 376.8MB 62.4MB/s eta 0:00:06     |████████████████▌               | 379.6MB 62.4MB/s eta 0:00:06     |██████████████████▏             | 416.1MB 30.4MB/s eta 0:00:11     |█████████████████████▏          | 486.3MB 47.1MB/s eta 0:00:06     |█████████████████████▉          | 501.8MB 32.7MB/s eta 0:00:08     |███████████████████████▎        | 534.1MB 54kB/s eta 1:01:21:59:14     |███████████████████████▊        | 543.7MB 54kB/s eta 0:58:25     |█████████████████████████▌      | 584.9MB 64.9MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from torch) (1.17.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting itertools\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 0, 'a': 1, 't': 2, 'M': 3}\n",
      "[3, 1, 2, 0, 3, 1, 2, 0, 3, 1, 2, 0, 3, 1, 2, 0, 3, 1, 2, 0]\n",
      "Weight Matrix for Wz:\n",
      " (tensor([[ 0.6614,  0.2669],\n",
      "        [ 0.0617,  0.6213],\n",
      "        [-0.4519, -0.1661],\n",
      "        [-1.5228,  0.3817]], requires_grad=True), torch.Size([4, 2])) \n",
      "\n",
      "Weight Matrix for Uz:\n",
      " (tensor([[ 0.3255, -0.4791],\n",
      "        [ 1.3790,  2.5286]], requires_grad=True), torch.Size([2, 2])) \n",
      "\n",
      "Weight Matrix for h_t_demo:\n",
      " (tensor([[0., 0.],\n",
      "        [0., 0.]]), torch.Size([2, 2])) \n",
      "\n",
      "Bias vector for bz: \n",
      " (tensor([0., 0.], requires_grad=True), torch.Size([2])) \n",
      "\n",
      "h0:tensor([[ 0.7565, -0.3472],\n",
      "        [-0.1355, -0.2040]], grad_fn=<AddBackward0>)\n",
      "h1:tensor([[-0.1535, -0.5712],\n",
      "        [ 0.7664, -0.5062]], grad_fn=<AddBackward0>)\n",
      "h2:tensor([[ 0.7495, -0.8616],\n",
      "        [-0.2399, -0.6680]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3da493017175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3da493017175>\u001b[0m in \u001b[0;36msample\u001b[0;34m(primer, length_chars_predict)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_chars_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcharacter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0minput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# This will be our input ---> x\n",
    "text = 'MathMathMathMathMath'\n",
    "\n",
    "character_list = list(set(text))   # get all of the unique letters in our text variable\n",
    "vocabulary_size = len(character_list)   # count the number of unique elements\n",
    "character_dictionary = {'h': 0, 'a': 1, 't': 2, 'M': 3}\n",
    "# {char:e for e, char in enumerate(character_list)}  # create a dictionary mapping each unique char to a number\n",
    "encoded_chars = [character_dictionary[char] for char in text] #integer representation of our vocabulary\n",
    "\n",
    "print(character_dictionary)\n",
    "print(encoded_chars)\n",
    "def one_hot_encode(encoded, vocab_size):\n",
    "    result = torch.zeros((len(encoded), vocab_size))\n",
    "    for i, idx in enumerate(encoded):\n",
    "        result[i, idx] = 1.0\n",
    "    return result\n",
    "\n",
    "# One hot encode our encoded charactes\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "num_samples = (len(encoded_chars) - 1) // seq_length # time lag of 1 for creating the labels\n",
    "vocab_size = 4\n",
    "\n",
    "data = one_hot_encode(encoded_chars[:seq_length*num_samples], vocab_size).reshape((num_samples, seq_length, vocab_size))\n",
    "num_batches = len(data) // batch_size\n",
    "X = data[:num_batches*batch_size].reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "X = X.transpose(1, 2)\n",
    "X.shape\n",
    "# However, we typically need the batch to be first so we would need to reshape our dataset --. S x B x V\n",
    "X, X.shape\n",
    "\n",
    "# +1 shift the labels by one so that given the previous letter the char we should predict would be or next char\n",
    "labels = one_hot_encode(encoded_chars[1:seq_length*num_samples+1], vocab_size)\n",
    "y = labels.reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "y = y.transpose(1, 2) # transpose the first and second index\n",
    "y,y.shape\n",
    "\n",
    "torch.manual_seed(1) # reproducibility\n",
    "\n",
    "####  Define the network parameters:\n",
    "hiddenSize = 2 # network size, this can be any number (depending on your task)\n",
    "numClass = 4 # this is the same as our vocab_size\n",
    "\n",
    "#### Weight matrices for our inputs\n",
    "Wz = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True)\n",
    "Wr = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True)\n",
    "Wh = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True)\n",
    "\n",
    "## Intialize the hidden state\n",
    "# this is for demonstration purposes only, in the actual model it will be initiated during training a loop over the\n",
    "# the number of bacthes and updated before passing to the next GRU cell.\n",
    "h_t_demo = torch.zeros(batch_size, hiddenSize)\n",
    "\n",
    "#### Weight matrices for our hidden layer\n",
    "Uz = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "Ur = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "Uh = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "\n",
    "#### bias vectors for our hidden layer\n",
    "bz = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "br = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "bh = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "\n",
    "#### Output weights\n",
    "Wy = Variable(torch.randn(hiddenSize, numClass), requires_grad=True)\n",
    "by = Variable(torch.zeros(numClass), requires_grad=True)\n",
    "\n",
    "print(f'Weight Matrix for Wz:\\n {Wz,Wz.size()}','\\n')\n",
    "print(f'Weight Matrix for Uz:\\n {Uz, Uz.size()}','\\n')\n",
    "print(f'Weight Matrix for h_t_demo:\\n {h_t_demo,h_t_demo.size()}','\\n')\n",
    "print(f'Bias vector for bz: \\n {bz, bz.size()}','\\n')\n",
    "torch.matmul(X[0][0], Wz) # This can be easily handled in pytorch using the mm or matmul(handles broadcasting)\n",
    "torch.matmul(Uz, h_t_demo), Uz\n",
    "bz\n",
    "z_inner = torch.matmul(X,Wz) +  torch.matmul(Uz,h_t_demo) + bz\n",
    "z_inner[0][0]\n",
    "z = torch.sigmoid(z_inner)\n",
    "z[0][0]\n",
    "\n",
    "r = torch.sigmoid(torch.matmul(X,Wr) +  torch.matmul(Ur,h_t_demo) + br)\n",
    "r[0][0]  # first bactch sequence\n",
    "torch.matmul(X[0][0], Wh)\n",
    "torch.matmul(r[0][0] * h_t_demo, Uh)\n",
    "bh\n",
    "h_inner_tilde = torch.matmul(X,Wh) +  r*torch.matmul(Uh,h_t_demo) + bh\n",
    "h_inner_tilde[0][0]\n",
    "\n",
    "h_tilde = torch.tanh(h_inner_tilde)\n",
    "h_tilde[0]\n",
    "\n",
    "ht_1 = z *h_t_demo + (1-z)* h_tilde\n",
    "ht_1[0][0]\n",
    "# h gets updated and then we calculate for the next\n",
    "h_t_1 = []\n",
    "h = h_t_demo\n",
    "for i,sequence in enumerate(X[0]):   # iterate over each sequence in the batch to calculate the hidden state h\n",
    "    z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "    r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "    h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "    h = z * h + (1 - z) * h_tilde\n",
    "    h_t_1.append(h)\n",
    "    print(f'h{i}:{h}')\n",
    "h_t_1 = torch.stack(h_t_1)\n",
    "h_t_minus_1 = torch.tensor([[ 0.7565, -0.3472],\n",
    "        [-0.1355, -0.2040]])\n",
    "X[0][0], X[0][1] # second sequence in batch 1\n",
    "h = h_t_minus_1\n",
    "z = torch.sigmoid(torch.matmul(X[0][1], Wz) + torch.matmul(h, Uz) + bz)\n",
    "r = torch.sigmoid(torch.matmul(X[0][1], Wr) + torch.matmul(h, Ur) + br)\n",
    "h_tilde = torch.tanh(torch.matmul(X[0][1], Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "hh = z * h + (1 - z) * h_tilde\n",
    "hh\n",
    "ht_2 = [] # stores the calculated h for each input x\n",
    "h = torch.zeros(batch_size, hiddenSize) # intitalizes the hidden state\n",
    "for batch in range(num_batches):  # this loops over the batches\n",
    "    x = X[batch]\n",
    "    for sequence in x: # iterates over the sequences in each batch\n",
    "        z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "        r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "        h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "        ht_2.append(h)\n",
    "ht_2 = torch.stack(ht_2)\n",
    "ht_2\n",
    "# This is the same\n",
    "fully_connected = torch.matmul(ht_2, Wy) + by\n",
    "fully_connected[0]\n",
    "fully_connected\n",
    "ylin_max = (fully_connected - fully_connected.max()) # first sequence in batch 1\n",
    "ylin_max[0]\n",
    "exp = ylin_max.exp()\n",
    "exp[0]\n",
    "exp_sum = exp[0].sum(dim=1,keepdim=True).reshape(-1,1) # the max value in each element within the sequence\n",
    "exp_sum\n",
    "exp[0]/exp_sum\n",
    "ht_2 = []  # stores the calculated h for each input x\n",
    "outputs = []\n",
    "h = torch.zeros(batch_size, hiddenSize)  # intitalizes the hidden state\n",
    "for i in range(num_batches):  # this loops over the batches\n",
    "    x = X[i]\n",
    "    for i, sequence in enumerate(x):  # iterates over the sequences in each batch\n",
    "        z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "        r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "        h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "\n",
    "        # Linear layer\n",
    "        y_linear = torch.matmul(h, Wy) + by\n",
    "\n",
    "        # Softmax activation function\n",
    "        y_t = F.softmax(y_linear, dim=1)\n",
    "\n",
    "        ht_2.append(h)\n",
    "        outputs.append(y_t)\n",
    "\n",
    "ht_2 = torch.stack(ht_2)\n",
    "outputs = torch.stack(outputs)\n",
    "outputs\n",
    "\n",
    "hh = torch.tensor([[ 0.7948, -0.8400],\n",
    "         [-0.3061, -0.7358]])\n",
    "fcc = torch.matmul(hh, Wy) + by\n",
    "F.softmax(fcc, dim=1)\n",
    "\n",
    "def gru(x, h):\n",
    "    outputs = []\n",
    "    for i,sequence in enumerate(x): # iterates over the sequences in each batch\n",
    "        z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "        r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "        h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "\n",
    "        # Linear layer\n",
    "        y_linear = torch.matmul(h, Wy) + by\n",
    "\n",
    "        # Softmax activation function\n",
    "        y_t = F.softmax(y_linear, dim=1)\n",
    "\n",
    "        outputs.append(y_t)\n",
    "    return torch.stack(outputs), h\n",
    "\n",
    "\n",
    "def sample(primer, length_chars_predict):\n",
    "    word = primer\n",
    "\n",
    "    primer_dictionary = [character_dictionary[char] for char in word]\n",
    "    test_input = one_hot_encode(primer_dictionary, vocab_size)\n",
    "\n",
    "    h = torch.zeros(1, hiddenSize)\n",
    "\n",
    "    for i in range(length_chars_predict):\n",
    "        outputs, h = gru(test_input, h)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].numpy())\n",
    "        word += character_list[choice]\n",
    "        input_sequence = one_hot_encode([choice], vocab_size)\n",
    "    return word\n",
    "\n",
    "\n",
    "max_epochs = 10  # passes through the data\n",
    "for e in range(max_epochs):\n",
    "    h = torch.zeros(batch_size, hiddenSize)\n",
    "    for i in range(num_batches):\n",
    "        x_in = X[i]\n",
    "        y_in = y[i]\n",
    "\n",
    "        out, h = gru(x, h)\n",
    "        print(sample('Ma', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
